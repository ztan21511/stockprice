{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kevin Nguyen, Lucas Woo, Andy Chen, Zachary Tan\n",
    "#### March 12, 2019\n",
    "#### INFO 370 Project\n",
    "\n",
    "\n",
    "\n",
    "## **Overview**\n",
    "The purpose of our research project is to be able to perform predictions on a particular company's stock return to enable investors to be more confident as to where they invest their money. Given the volatile nature of the stock market, we knew this is an extremely difficult task due to a variety of factors, from environmental factors to emotional factors. Our goal is to inform people of a model that can help predict stock returns and filter out which factors are more significant than others in making these predictions. This can ultimately allow novice investors to make shrewd stock market decisions. In our given dataset, we were given over 70 quantitative fundamental characteristics for a given company, and wanted to further analyze those specific characteristics to make predictions for future stock returns. Specifically, we want to answer the question: Do company financials and fundamental characteristics correlate with future stock returns?\n",
    "\n",
    "The dataset that we are working with is one that we found from <https://www.kaggle.com/dgawlik/nyse/version/3#_=_>. Something to note about the dataset used is that these are typically more established companies and a dataset including smaller, newer companies could see more volatility in returns. The person who scraped the data fetched the prices from Yahoo Finance and grabbed the fundamental columns from Nasdaq Financials and some of those columns were extended using fields from the EDGAR SEC databases. The data of interest was seperated into two datasets, one that contained the quantitative fundamental characteristics for a given company on a given day and another that contained the opening stock prices for a given company on a given day. The data was merged via a self-implemented ID column. \n",
    "\n",
    "\n",
    "*Abbreviated sample of our cleaned dataset. Note: open represents annualized stock return*\n",
    "\n",
    "\n",
    "![](img/sampledata.PNG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation\n",
    "To prepare our data and scale it we took a couple of different steps. Although we had multiple years of data for the different stocks in the dataset, we chose to either average the column or take the an annualized increase or decrease from first to last year that we had data for the stock. This enabled us to have the same number of rows (1) for each stock in our dataset. For columns that were ratios we took the mean ratio over the years we had data for and used that for the company row. For the others, we took an annualized percentage change by taking the last year value, subtracting the first year value, and then dividing by the number of years of data. This gave us a common scale of one year returns and also gave them equal weights. This was also done for our outcome variable as we converted opening price to the return of the stock from the first year of data to the last year of data. We didn't convert ratios to yearly percent changes as the ratios are already scaled for us.\n",
    "\n",
    "In addition to the previous measures taken, we also added a column of interest. We chose to add in company size in terms of revenue instead of market capitalization since stock price is used when calculating market capitalization which is our outcome variable. We broke these up into small, medium, and large firms. Historically, small firms have outperformed large firms.\n",
    "\n",
    "For NaN values, we decided to use the the company who has the next closest gross margin to that company and use their value for the column with the undefined value if they do not have an NaN value as well for that column. In the scenario where they were also missing that value we would continue searching for the company who has a value in the column of interest and had the next closest gross margin. We chose to use gross margin because companies with similar profit margins also have similar company financial fundamentals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## **Discovering Correlation**\n",
    "\n",
    "\n",
    "##### **Correlationship Map for All Fundamentals**\n",
    "![](img/heatmap.png)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "From the fundamental dataset we were given, we wanted to know which fundamental variables potentially correlate to a stock's return and the strength of each fundamental individually. Above is a heatmap to indicate the amount of strength in correlation each fundamental variable has with one another. Note that the correlation visuals are in terms of absolute values.\n",
    "\n",
    "\n",
    "\n",
    "##### **Correlationship Rankings For All Fundamentals to Returns**\n",
    "![](img/rankings.png)\n",
    "\n",
    "\n",
    "\n",
    "Since return is the variable under analysis, above is a more concise visualization that hones in on the strength of correlation between returns and the fundamental variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Univariable Regression anlysis**\n",
    "\n",
    "In order to find the relationship between each factor and stock return, we first did a univariable regression analysis. The results are shown below:\n",
    "![](img/singlevariate.png)\n",
    "\n",
    "The above graph ranks the factors based on the p-value. The factor that has the smallest p-value is shown at the top. According to the above graph, there are 18 factors that have a p-value smaller than 0.05. They are gross margin, current ratio, total revenue, quick ratio, total asssets, cash ratio, earnings per share, operating margin, pre-tax margin, total liabilities, company size, after tax ROE, pre-tax ROE, earning before interest and tax, long-term investments, total current liabilities, operating income, gross profit, net receivables, accounts payable. It means the relationship between these factors and stock returns are statistically significant. However, the r squared values for these factors all smaller than 27%, which means it is hard to illustrate relationships by using only one variable.\n",
    "\n",
    "\n",
    "\n",
    "## **Multivariate Regression**\n",
    "\n",
    "Like mentioned above, the result of r-squared values are small from the univariate regression. So, it is worth it to do the multivariate regression analysis. By using the multivariate regression, this better reflects more real life situations and thus is a better representation of how much of a stock return can be attributed to these factors.\n",
    "\n",
    "When running the multivariate regression with our chosen features, we received an r-squared value of .241. This indicates that only 24.1% of the variation of stock return can be attributed to the factors we chose. This could mean that by adding as so many factors we created extra noise to the model. Another interesting observation was that the only statisically signifcant factors were company size, total revenue, total liabilities, and total assets.\n",
    "\n",
    "*Abbreviated sample of our OLS Regression Results*\n",
    "![](img/multivariate.png)\n",
    "\n",
    "![](img/olssummary.png)\n",
    "\n",
    "\n",
    "### **Forward Selection**\n",
    "To not overfit the model, we must account for variables that have a statistically non-significant relations with our outcome variable.\n",
    "We defined a formula for forward selection. The formula is an \"optimal\" fitted statsmodels polynomial linear model and tries to find the model that has the best adjusted R-squared.\n",
    "\n",
    "When we did our forward selection, we received an r squared of .439, significantly better than the multivariate. By applying the forward selection,  we continually add different factors to the model until the model has the best r-adjusted value (the value shows how much variation are accounted by the model). \n",
    "\n",
    "The resulting \"best fit\" model is now reduced to 13 fundamental variables, and the predictions are plotted below:\n",
    "![](img/feature.png)\n",
    "\n",
    "![](img/best_fit-1.png)\n",
    "\n",
    "The above graph shows the best model we got actually is not so accurate. The x-axis represents the value the actual stock returns, and y-axis represents the prediction we made. The model is bad at making the prediction for the stock that has actually has the return from 0 to 0.2. However, most of stocks have return from 0 to 0.2. So, the model we created canâ€™t accurately predict for most of the cases.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Machine Learning Models**\n",
    "\n",
    "### Factors Used\n",
    "For our models, we decided to use the same factors we used in our multivariate regression instead of the feature selection factors. This is because of the flexibility that using our previous selected factors gives us as well as takes away the chance that feature selection missed out on important factors. Ultimately, when we tried models with feature selection factors instead of our multivariate regression models, we performed **worse**.\n",
    "\n",
    "### K Neighbors\n",
    "Our first model used was the K Nearest Neighbors Classifier. K Nearest Neighbors uses the closest N neighbors in order to make predictions on a data set. We identified two hyper parameters to tune, n_neighbors and the weight to be used. The scores we recieved on all our models are classifed as the negative mean absolute score which identifies the average difference between our predictions and the actual stocks return. In this model, we recieved a score of **-0.1225** which means that we were off by **-12.25%** of the annualized stock return. This was actually our worst model out of all of them. \n",
    "![](img/knn.png)\n",
    "\n",
    "\n",
    "### Decision Tree\n",
    "The second model used was Decision Tree Regression. Because our dataset is not wholly linear, contrary to K Neighbors, using a nonparametric method is appropriate to account for any nonlinear structure in the underlying distribution. Using the features that are inputted into the model, it creates a tree that has the ability to jump to conclusions about an item's target value. For our model, we recieved a negative mean score of **-0.1183**. \n",
    "![](img/decision.png)\n",
    "\n",
    "### Random Forest\n",
    "Similar to the K Nearest Neighbors Classifier, the Random Forest Classifier can also be considered a weighted neighbor model. It differs from the Decision Tree model because it creates multiple decision trees, which then uses the mean of those individual trees. The negative mean absolute score we got for this model was **-0.1092**, this means that the predictions for this model were the best out of all the models we used. \n",
    "![](img/forest.png)\n",
    "\n",
    "### Kernel Ridge\n",
    "Our final model uses ridge regression. This model is especially good at preventing multicollinearity from affecting the outcome of the model. We felt this was an appropriate model to use due to the close nature many of the filtered fundamentals share. Finally, for this model we recieved a score of **-0.1118**. \n",
    "![](img/kernel.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After conducting our models, we realized it was very difficult to predict negative returns and and also large positive returns. Our best model was only able to predict within -.1092 of a stock's return. As previously noted in academic journals and financial firms, it is **extremely** hard to predict a stock return because of the volatility and unpredictability. According to the univariate regression analysis, there are over 18 factors related to the stock returns that is not observed by chance. All in all, the models we got from our machine learning models and statical approaches were not accurate enough to predict the stock returns in real life. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
